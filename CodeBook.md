# ðŸ“š Codebook: Stand-up Comedy Transcript Analysis  

## 1âƒ£ Dataset Overview  
This dataset consists of **transcripts from 20 stand-up comedians**, scraped from [Scraps from the Loft](https://scrapsfromtheloft.com/stand-up-comedy-transcripts/). It is used to analyze vocabulary, common words, and profanity usage across different comedians. The analysis focuses on text-based insights rather than tone, delivery, or sentiment.

## 2âƒ£ Data Collection  
- **Source**: Transcripts were collected from "Scraps from the Loft," a website that provides scripts of various stand-up comedy performances.
- **Method**: Web scraping was used to extract text from comedian-specific pages.
- **Preprocessing**: The raw text was stored and cleaned before analysis.

## 3âƒ£ Variables & Features  

### ðŸ”¹ Raw Data Variables  
- **comedian_name** â€“ Name of the stand-up comedian.  
- **transcript** â€“ Full text of their stand-up routine.  

### ðŸ”¹ Processed Data Variables  
- **cleaned_transcript** â€“ Text after removing special characters and extra spaces.  
- **word_count** â€“ Total number of words in the transcript.  
- **unique_words** â€“ Number of unique words used.  
- **f_word_count** â€“ Number of times the comedian used the **F-word**.  
- **s_word_count** â€“ Number of times the comedian used the **S-word**.  
- **one_hot_encoding** â€“ A binary matrix representing the presence of words in the transcript.  
- **most_common_words** â€“ List of most frequently used words by the comedian.  
- **unique_words_per_comedian** â€“ Words that appear frequently for one comedian but rarely for others.  
- **stop_words_removed** â€“ Processed text with common stop words filtered out.

## 4âƒ£ Data Cleaning & Processing  
- **Regular Expressions** were used to remove unwanted characters, punctuation, and extra spaces.  
- **One-Hot Encoding** was applied to convert words into binary format for analysis.  
- **WordCloud** was generated to visualize frequently used words.  
- **Stop Words Removal** was performed to filter out common words like *the, is, and*.  
- **Profanity Analysis** counted the occurrences of specific swear words.  
- **Pickling** was used to **store and reload processed data efficiently**.  
- **CountVectorizer** was used to extract word frequency counts and analyze trends in word usage.  
- **TF-IDF Analysis** was used to identify important words specific to each comedian.

## 5âƒ£ Analysis Methods  
- **Word Frequency Analysis** to find the most used words by each comedian.  
- **WordCloud Visualization** for an overview of the comedianâ€™s vocabulary.  
- **Profanity Analysis** to determine how frequently each comedian uses explicit words.  
- **Comparison Across Comedians** to identify stylistic and linguistic patterns.  
- **Unique Word Analysis** to detect words that define each comedian's style.  
- **TF-IDF Analysis** to highlight words uniquely important to each comedian.  

## 6âƒ£ Output Files  
- **cleaned_transcripts.csv** â€“ Preprocessed transcripts after cleaning.  
- **wordclouds/** â€“ Folder containing WordCloud images for each comedian.  
- **profanity_counts.csv** â€“ A CSV file with the count of explicit words for each comedian.  
- **final_analysis.csv** â€“ Processed dataset with all extracted features.  
- **pickled_data.pkl** â€“ **Pickled Python object storing processed data for faster loading.**  
- **tfidf_results.csv** â€“ TF-IDF scores for words across different comedians.  

##
